fit_holt_linear = holt(temp, alpha = .8, beta = .2, damped = TRUE, initial = "optimal", h = 11)
fit_holt_exp = holt(temp, alpha = .8, beta = .2, damped = TRUE, initial = "optimal", exponential = TRUE, h = 11)
# estimated values using linear trend
fitted(fit_holt_linear)
# forecasted values usign linear trend
fit_holt_linear$mean
# estimated values using exponential trend
fitted(fit_holt_exp)
# forecasted values usign exponential trend
fit_holt_exp$mean
plot(temp,ylab = "Max Temperatures", xlab = "Year", type = "o", xlim = c(1990, 2021),ylim = c(35,50), main = "Max Annual Temp (Holt)")
lines(fitted(fit_holt_linear), col = "blue", type= "o")
lines(fit_holt_linear$mean,col = "blue", type= "o")
lines(fitted(fit_hold_exp), col = "green", type= "o")
lines(fit_hold_exp$mean,col = "green", type= "o")
# Holt's dampened Linear Trend Model for AUS AIR
fit_holt_linear = holt(temp, alpha = .8, beta = .2, damped = TRUE, initial = "optimal", h = 11)
# Holt dampened Exponential Model
fit_holt_exp = holt(temp, alpha = .8, beta = .2, damped = TRUE, initial = "optimal", exponential = TRUE, h = 11)
# estimated values using linear trend
fitted(fit_holt_linear)
# forecasted values usign linear trend
fit_holt_linear$mean
# estimated values using exponential trend
fitted(fit_holt_exp)
# forecasted values usign exponential trend
fit_holt_exp$mean
plot(temp,ylab = "Max Temperatures", xlab = "Year", type = "o", xlim = c(1990, 2021),ylim = c(35,50), main = "Max Annual Temp (Holt)")
# Plot the fitted value (estimated from triaining data)
lines(fitted(fit_holt_linear), col = "blue", type= "o")
# Plot the forecasts
lines(fit_holt_linear$mean,col = "blue", type= "o")
# Plot the fitted value (estimated from triaining data)
lines(fitted(fit_hold_exp), col = "green", type= "o")
#Plot the forecasts
lines(fit_hold_exp$mean,col = "green", type= "o")
# Plot the fitted value (estimated from triaining data)
lines(fitted(fit_holt_exp), col = "green", type= "o")
#Plot the forecasts
lines(fit_holt_exp$mean,col = "green", type= "o")
# with implicit Test set... it figures out by the time which are training and which are test.
accuracy(fit_holt_linear, maxtemp)
accuracy(fit_holt_exp, maxtemp)
#with explicit Test set ... (same output)
temp_comp_holt = window(maxtemp, start = 1990)
accuracy(fit_holt, temp_comp_holt)
accuracy(fit_hold_exp, temp_comp_holt)
#Add the actual values to visually compare forecasts to actual values
temp_actual = window(maxtemp, start = 1990)
points(temp_actual, type = "o")`
#Add the actual values to visually compare forecasts to actual values
temp_actual = window(maxtemp, start = 1990)
points(temp_actual, type = "o")`
temp_actual = window(maxtemp, start = 1990)
points(temp_actual, type = "o")
legend("topleft", lty=1, col=c(1, "blue", "red", "green"), c("original", expression("Linear"), expression("Exponential")), pch=1)
fit_holt_linear$model[2:3]
legend("topleft", lty=1, col=c(1, "blue", "green"), c("original", expression("Linear"), expression("Exponential")), pch=1)
plot(temp,ylab = "Max Temperatures", xlab = "Year", type = "o", xlim = c(1990, 2021),ylim = c(35,50), main = "Max Annual Temp (Holt)")
# Plot the fitted value (estimated from triaining data)
lines(fitted(fit_holt_linear), col = "blue", type= "o")
# Plot the forecasts
lines(fit_holt_linear$mean,col = "blue", type= "o")
# Plot the fitted value (estimated from triaining data)
lines(fitted(fit_holt_exp), col = "green", type= "o")
#Plot the forecasts
lines(fit_holt_exp$mean,col = "green", type= "o")
legend("topleft", lty=1, col=c(1, "blue", "green"), c("original", expression("Linear"), expression("Exponential")), pch=1)
fit_holt_linear$model[2:3]
fit_holt_exp$model[2:3]
#with explicit Test set ... (same output)
temp_comp_holt = window(maxtemp, start = 1990)
accuracy(fit_holt, temp_comp_holt)
accuracy(fit_hold_exp, temp_comp_holt)
#with explicit Test set ... (same output)
temp_comp_holt = window(maxtemp, start = 1990)
accuracy(fit_holt_linear, temp_comp_holt)
accuracy(fit_holt_exp, temp_comp_holt)
#with explicit Test set ... (same output)
temp_comp_holt = window(maxtemp, start = 1990)
accuracy(fit_holt_linear, temp_comp_holt)
accuracy(fit_holt_exp, temp_comp_holt)
#Add the actual values to visually compare forecasts to actual values
temp_actual = window(maxtemp, start = 1990)
points(temp_actual, type = "o")
help(maxtemp)
library(fpp)
data(ausair)
air = window(ausair, start = 1990, end = 2004)
plot(air,ylab = "Airline Passegners", xlab = "Year", main = "Airline Passengers")
fit1 = ses(air, initial = "simple",alpha = .2,h = 3)
fit2 = ses(air, initial = "simple",alpha = .6, h = 3)
fit3 = ses(air, h = 3) #defaults
accuracy(fit1, ausair)
accuracy(fit2, ausair)
accuracy(fit3, ausair)
plot(air,ylab = "Airline Passegners", xlab = "Year", type = "o", xlim = c(1990, 2008),ylim = c(15,50), main = "Airline Passengers")
#Plot the estimated values from the models .. the "fitted" values are the training values.
lines(fitted(fit1), col = "blue", type = "o")
lines(fitted(fit2), col = "red", type = "o")
lines(fitted(fit3), col = "green", type = "o")
# the  $mean values are the forecasts.
lines(fit1$mean, col = "blue", type = "o")
lines(fit2$mean, col = "red", type = "o")
lines(fit3$mean, col = "green", type = "o")
# These are the actual values!  Compare visually with the forecasts!
air2008 = window(ausair, start = 1990, end = 2007)
points(air2008, type = "o")
# Compare the forecasts with the actual values with various fit metrics.
accuracy(fit1, air2008)
accuracy(fit2, air2008)
accuracy(fit3, air2008)
library(fpp2)
data(maxtemp) #1971-2016
temp <- window(maxtemp, start=1990, end=2010)
plot(temp,ylab = "Max Temperature", xlab = "Year", main = "Max Annual Temp (SES)")
# fitting exponential smoothing forecast model for up to 8 years
temp_fit1 = ses(temp, initial = "optimal",alpha = .2,h = 11)
temp_fit2 = ses(temp, initial = "optimal",alpha = .6, h = 11)
temp_fit3 = ses(temp, h = 11) #defaults
temp_fit1
temp_fit1$model
temp_fit1 = ses(temp, initial = "simple",alpha = .2,h = 11)
temp_fit1$model
data(ausair)
ausair
install.packages(c("bestglm", "gplots", "ResourceSelection", "ROCR"))
install.packages(c("epitools", "samplesizeCMH"))
prop.test(335,411,p=.7,correct=TRUE)  #correct is the continuity correction option
vit.c<-data.frame(Supp=rep(c("Placebo","Vitamin C"),times=c(411,407)),
Cold=rep(c("Yes","No","Yes","No"),times=c(335,76,302,105)))
head(vit.c)
mymat<-table(vit.c)   # or table(vit.c$Supp,vit.c$Cold)
mymat
mymat_2<-matrix(c(76,335,105,302),2,2,byrow=T,dimnames=list(c("Placebo","Vitamin C"),c("No","Yes")))
mymat_2
chisq.test(mymat,correct=TRUE)
prop.table(mymat,margin=1)
prop.test(335,411,p=.7,correct=TRUE)  #correct is the continuity correction option
prop.test(mymat,correct=TRUE)
prop.test(mymat[c(2,1),c(2,1)],correct=TRUE)
library(epitools)
mymat<-matrix(c(76,335,105,302),2,2,byrow=T)
mymat<-matrix(c(76,335,105,302),2,2,byrow=T)
mymat
dimnames(mymat)<-list("Treatment"=c("Plac","Vit C"),"Response"=c("No","Yes"))
dimnames
mymat
mymat<-matrix(c(76,335,105,302),2,2,byrow=T)
mymat
dimnames(mymat)<-list("Treatment"=c("Plac","Vit C"),"Response"=c("No","Yes"))
mymat
#Odds Ratio Intervals
oddsratio.wald(mymat)
#Relative Risk Intervals
riskratio.wald(mymat)
library(epitools)
#Another way to format a count matrix
mymat<-matrix(c(76,335,105,302),2,2,byrow=T)
dimnames(mymat)<-list("Treatment"=c("Plac","Vit C"),"Response"=c("No","Yes"))
mymat
#Odds Ratio Intervals
oddsratio.wald(mymat)
#Relative Risk Intervals
riskratio.wald(mymat)
library(epitools)
#Another way to format a count matrix
mymat<-matrix(c(76,335,105,302),2,2,byrow=T)
mymat
dimnames(mymat)<-list("Treatment"=c("Plac","Vit C"),"Response"=c("No","Yes"))
mymat
#Odds Ratio Intervals
oddsratio.wald(mymat)
mymat
oddsratio.wald(mymat, rev="rows") #
oddsratio.wald(mymat, rev="columns") # results in placebo(N)/vitamin(Y)
oddsratio.wald(mymat) # results in placebo(N)/vitamin(Y)
oddsratio.wald(mymat, rev="rows") # results in placebo(Y)/Vitamin(N)
oddsratio.wald(mymat, rev="columns") # results in placebo(N)/vitamin(Y)
riskratio.wald(mymat)
riskratio.wald(mymat)
riskratio.wald(mymat, rev="rows")
riskratio.wald(mymat, rev="columns")
breast_cancer_df <-matrix(c(330,658,204,386),2,2,byrow=T)
breast_cancer_df
dimnames(breast_cancer_df)<-list("Case Type"=c("< 4 Drinks/week","> 4 Drink/week"),"Response"=c("Cases","Controls"))
breast_cancer_df
oddsratio.wald(breast_cancer_df) # results in placebo(N)/vitamin(Y)
oddsratio.wald(breast_cancer_df, rev="rows") # results in placebo(Y)/Vitamin(N)
oddsratio.wald(breast_cancer_df, rev="columns") # results in placebo(N)/vitamin(Y)
riskratio.wald(mymat)
riskratio.wald(mymat, rev="rows")
riskratio.wald(mymat, rev="columns")
oddsratio.wald(breast_cancer_df, rev="rows") # results in placebo(Y)/Vitamin(N)
oddsratio.wald(breast_cancer_df, rev="columns") # results in placebo(N)/vitamin(Y)
oddsratio.wald(breast_cancer_df) # results in placebo(N)/vitamin(Y)
oddsratio.wald(breast_cancer_df, rev="rows") # results in placebo(Y)/Vitamin(N)
oddsratio.wald(breast_cancer_df, rev="columns") # results in placebo(N)/vitamin(Y)
riskratio.wald(mymat, rev="columns")
riskratio.wald(mymat)
riskratio.wald(mymat, rev="rows")
riskratio.wald(mymat, rev="columns")
#Relative Risk Intervals
riskratio.wald(breast_cancer_df)
riskratio.wald(breast_cancer_df, rev="rows")
riskratio.wald(breast_cancer_df, rev="columns")
oddsratio.wald(breast_cancer_df) # results in placebo(N)/vitamin(Y)
riskratio.wald(mymat, rev="columns")
install.packages("aws.s3")
library(tidyr)
library(dplyr)
library(ggplot2)
wd <- "C:/Users/David/Google Drive/Code & Programming/GitHub/MSDS_6372_Applied-Statistics_Project_2/UCI_Bank_Marketing_Classification_Model/Data Files/bank-additional-full.csv"
setwd(wd)
install.packages("gmodels")
library(tidyr)
library(dplyr)
library(ggplot2)
library(tidyverse)
setwd("C:/Users/David/Google Drive/Code & Programming/GitHub/MSDS_6372_Applied-Statistics_Project_2/UCI_Bank_Marketing_Classification_Model/Data Files/")
df <- read.csv("bank-additional-full.csv", header=TRUE, sep=";")
str(df)
# renaming response variable 'y' to 'Clnt_Subcr'
df <- df %>% rename(clnt.subcr = `y`)
library(naniar)
# verifying missing data: no missing data found
vis_miss(df) + xlab("Data Columns")
# adding sequential ID column to all rows
df$ID <- seq.int(nrow(df))
df <- df %>% select(ID, everything())
# finding which columns in df are categorical
cat_class <- as.data.frame(sapply(df, class))
cat_class
# exploring the unique data values for all categorical variables
cat_types <- c(3,4,5,6,7,8,9,10,11,16,22)
for(i in cat_types){
print(colnames(df[i]))
print(df %>% count(df[,i]))
}
df_stg <- df
df_stg[df_stg=="unknown"] <- NA
vis_miss(df_stg, warn_large_data = FALSE) + xlab("Data Columns")
# new df storing ID and all converted categorical variables into factor types
df_cat <- df_stg
for(i in cat_types){
df_cat[, i] <- factor(df_cat[,i])
x <- paste(colnames(df_cat[i]),"desc", sep="_")
colnames(df_cat)[i] <- paste(x)
}
# create new df containing ID and all converted categorical variables into numerical types
df_cat_num <- df_stg
for(i in cat_types){
df_cat_num[, i] <- as.numeric(factor(df_cat_num[,i]))
x <- paste(colnames(df_cat_num[i]),"ID", sep="_")
colnames(df_cat_num)[i] <- paste(x)
}
cat_columns <- c(1,3,4,5,6,7,8,9,10,11,16,22)
df_cat_num_short <- data.frame(df_cat_num[, cat_columns])
df_cat_short <- data.frame(df_cat[, cat_columns])
# storying all numerical data into temp df excluding categorical types
df_num_only <- df[, -cat_types]
# joining the categorical factors (desc) with it's numerical types (IDs) then re-ordering
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
df_cat_merge <- df_cat_merge[c(1,2,13,3,14,4,15,5,16,6,17,7,18,8,19,9,20,10,21,11,22,12,23)]
# joining all numerical with categorical data together as final df
df_final <- merge(x=df_cat_merge, y=df_num_only, by="ID", all.x=TRUE)
head(df_final, 1)
# validating df matches all prior df stages
nrow(df) #raw
nrow(df_stg) #filterd NA
nrow(df_final)
set.seed(27)
splitPerc = .70
df_index = sample(1:dim(df_final)[1],round(splitPerc * dim(df_final)[1]))
df_train = df_final[df_index,]
df_test = df_final[-df_index,]
# Train/Test data with Downsampling Applied
library(caret)
set.seed(27)
df_train_dwnsmpled <- downSample(x = df_train,y = df_train$clnt.subcr_desc)
df_test_dwnsmpled <- downSample(x = df_test,y = df_test$clnt.subcr_desc)
table(df_train$clnt.subcr_ID) #train nondownsampled
table(df_test$clnt.subcr_ID) #test nondownsample
table(df_final$clnt.subcr_ID) #cleaned table
table(df_train_dwnsmpled$clnt.subcr_ID) #train downsampled
table(df_test_dwnsmpled$clnt.subcr_ID) #test downsampled
library(GGally)
library(corrplot)
library(stringr)
library(MASS)
exclude_columns_EDA <- c(1,3,5,7,8,9,12,13,15,17,18,19,21,34)
# removing the EDA based attributes to both the downsampled test and train set
full_model_train <- df_train_dwnsmpled[, -exclude_columns_EDA]
full_model_test <- df_test_dwnsmpled[, -exclude_columns_EDA]
#validating column consistency
ncol(full_model_train)
ncol(full_model_test)
nrow(full_model_train)
nrow(full_model_test)
full_model <- lm(clnt.subcr_ID~., data=full_model_train[, -10])
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
full_model <- lm(clnt.subcr_ID~., data=full_model_train[, -10])
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
summary(step_model)
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
# removing the EDA based attributes to both the downsampled test and train set
full_model_train <- df_train_dwnsmpled[, -exclude_columns_EDA]
full_model_test <- df_test_dwnsmpled[, -exclude_columns_EDA]
full_model <- lm(clnt.subcr_ID~., data=full_model_train[, -10])
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
summary(step_model)
exclude_columns_EDA <- c(1,3,5,7,8,9,12,13,15,17,18,19,21,34)
# removing the EDA based attributes to both the downsampled test and train set
# removing the EDA based attributes to both the downsampled test and train set
full_model_train <- df_train_dwnsmpled[, -exclude_columns_EDA]
full_model_test <- df_test_dwnsmpled[, -exclude_columns_EDA]
nrow(full_model_train)
nrow(full_model_test)
ncol(full_model_train)
ncol(full_model_test)
full_model <- lm(clnt.subcr_ID~., data=full_model_train[, -10])
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
summary(step_model)
fwd_model <- stepAIC(full_model, direction="forward", trace=FALSE)
summary(fwd_model)
bck_model <- stepAIC(full_model, direction="backward", trace=FALSE)
# converting all "unknown" values back to NA
df_stg <- df
df_stg[df_stg=="unknown"] <- NA
vis_miss(df_stg, warn_large_data = FALSE) + xlab("Data Columns")
# new df storing ID and all converted categorical variables into factor types
df_cat <- df_stg
for(i in cat_types){
df_cat[, i] <- factor(df_cat[,i])
x <- paste(colnames(df_cat[i]),"desc", sep="_")
colnames(df_cat)[i] <- paste(x)
}
# create new df containing ID and all converted categorical variables into numerical types
df_cat_num <- df_stg
for(i in cat_types){
df_cat_num[, i] <- as.numeric(factor(df_cat_num[,i]))
x <- paste(colnames(df_cat_num[i]),"ID", sep="_")
colnames(df_cat_num)[i] <- paste(x)
}
cat_columns <- c(1,3,4,5,6,7,8,9,10,11,16,22)
df_cat_num_short <- data.frame(df_cat_num[, cat_columns])
df_cat_short <- data.frame(df_cat[, cat_columns])
# storying all numerical data into temp df excluding categorical types
df_num_only <- df[, -cat_types]
# joining the categorical factors (desc) with it's numerical types (IDs) then re-ordering
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
df_cat_merge <- df_cat_merge[c(1,2,13,3,14,4,15,5,16,6,17,7,18,8,19,9,20,10,21,11,22,12,23)]
# joining all numerical with categorical data together as final df
df_final <- merge(x=df_cat_merge, y=df_num_only, by="ID", all.x=TRUE)
head(df_final, 1)
set.seed(27)
splitPerc = .70
df_index = sample(1:dim(df_final)[1],round(splitPerc * dim(df_final)[1]))
df_train = df_final[df_index,]
df_test = df_final[-df_index,]
set.seed(27)
df_train_dwnsmpled <- downSample(x = df_train,y = df_train$clnt.subcr_desc)
df_test_dwnsmpled <- downSample(x = df_test,y = df_test$clnt.subcr_desc)
df_train_dwnsmpled$month_desc <- str_to_title(df_train_dwnsmpled$month_desc)
df_train_dwnsmpled$month_desc <- factor(df_train_dwnsmpled$month_cap, levels=month.abb)
df_train_dwnsmpled$month_desc <- str_to_title(df_train_dwnsmpled$month_desc)
df_train_dwnsmpled$month_desc <- factor(df_train_dwnsmpled$month_cap, levels=month.abb)
# number of campaigns run per month
df_train_dwnsmpled %>% ggplot(aes(x=month_desc, y=campaign,fill=day_of_week_desc)) + geom_bar(stat="identity") + scale_x_discrete(limits=month.abb) + labs(title="# of Campaigns per Month by Day", x="Month", y="# of Campaigns", fill="Day of Week")
exclude_columns_EDA <- c(1,3,5,7,8,9,12,13,15,17,18,19,21,34)
# removing the EDA based attributes to both the downsampled test and train set
full_model_train <- df_train_dwnsmpled[, -exclude_columns_EDA]
full_model_test <- df_test_dwnsmpled[, -exclude_columns_EDA]
full_model <- lm(clnt.subcr_ID~., data=full_model_train[, -10])
step_model <- stepAIC(full_model, direction="both", trace=FALSE)
summary(step_model)
logit_model <- glm(clnt.subcr_desc~., family="binomial", data=full_model_train[, -9])
summary(logit_model)
logit_pred <- predict(logit_model, newdata=full_model_test, type="response")
cutoff <- .5
logit_pred.class <- factor(ifelse(logit_pred>cutoff, "yes", "no"))
confusionMatrix(logit_pred.class, as.factor(full_model_test$clnt.subcr_desc))
knn_train<-df_train_dwnsmpled
knn_test<-df_test_dwnsmpled
#Take class and remove from data set
knn_train<-knn_train[,c("job_ID", "marital_ID", "education_ID", "default_ID", "housing_ID", "loan_ID", "contact_ID", "month_ID", "day_of_week_ID", "poutcome_ID", "age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "clnt.subcr_ID")]
knn_test<-knn_test[,c("job_ID", "marital_ID", "education_ID", "default_ID", "housing_ID", "loan_ID", "contact_ID", "month_ID", "day_of_week_ID", "poutcome_ID", "age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "clnt.subcr_ID")]
library(MASS)
lda_train<-df_train_dwnsmpled
lda_test<-df_test_dwnsmpled
str(lda_train)
lda_train<-lda_train[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed")]
lda_test<-lda_test[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed")]
model.lda<-lda(clnt.subcr_ID~., data=lda_train)
lda_train<-lda_train[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed", "clnt.subcr_ID")]
lda_test<-lda_test[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed", "clnt.subcr_ID")]
lda_train<-lda_train[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed", "clnt.subcr_ID")]
lda_train<-df_train_dwnsmpled
lda_test<-df_test_dwnsmpled
lda_train<-lda_train[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed", "clnt.subcr_ID")]
lda_test<-lda_test[,c("age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed", "clnt.subcr_ID")]
ncol(lda_train)
model.lda<-lda(clnt.subcr_ID~., data=lda_train[,-11])
str(lda_train)
lda_train[,-11]
model.lda<-lda(clnt.subcr_ID~., data=lda_train[,-11])
model.lda<-lda(clnt.subcr_ID~., data=lda_train)
summary(model.lda)
model.lda
model.lda<-lda(clnt.subcr_ID~., data=lda_train)
predictions <- model.lda %>% predict(lda_train)
mean(predictions$class==lda_train$clnt.subcr_ID)
model.lda
plot(model.lda)
head(predictions$class, 6)
head(predictions$posterior, 6)
head(predictions$x, 3)
lda.data <- cbind(lda_train, predict(model.lda)$x)
ggplot(lda.data, aes(LD1, clnt.subcr_ID)) +
geom_point(aes(color = clnt.subcr_ID))
mean(predictions$class==lda_train$clnt.subcr_ID)
mean(predictions$class==lda_train$clnt.subcr_ID)
library(glmnet)
lso.train<-lda_train
lso.test<-lda_test
lso.train<-df_train_dwnsmpled
lso.tes<-df_test_dwnsmpled
str(lso.train)
lso.train<-df_train_dwnsmpled[,c("job_ID", "marital_ID", "education_ID", "default_ID", "housing_ID", "loan_ID", "contact_ID", "month_ID", "day_of_week_ID", "poutcome_ID", "age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "clnt.subcr_ID")]
lso.test<-df_test_dwnsmpled[,c("job_ID", "marital_ID", "education_ID", "default_ID", "housing_ID", "loan_ID", "contact_ID", "month_ID", "day_of_week_ID", "poutcome_ID", "age", "duration", "campaign", "pdays", "previous", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "clnt.subcr_ID")]
x_vars_train<-model.matrix(clnt.subcr_ID~., lso.train)[,-20]
y_var_train <- lso.train$clnt.subcr_ID
x_vars_test<-model.matrix(clnt.subcr_ID~., lso.test)[,-20]
y_var_test <- lso.test$clnt.subcr_ID
lambda_seq <- 10^seq(2, -2, by = -.1)
cv_output <- cv.glmnet(x_vars_train, y_var_train,
alpha = 1, lambda = lambda_seq,
nfolds = 5)
confusionMatrix(logit_pred.class, as.factor(full_model_test$clnt.subcr_desc))
library(tidyr)
library(dplyr)
library(ggplot2)
library(tidyverse)
setwd("C:/Users/David/Google Drive/Code & Programming/GitHub/MSDS_6372_Applied-Statistics_Project_2/UCI_Bank_Marketing_Classification_Model/Data Files/")
df <- read.csv("bank-additional-full.csv", header=TRUE, sep=";")
str(df)
df <- df %>% rename(clnt.subcr = `y`)
library(naniar)
# adding sequential ID column to all rows
df$ID <- seq.int(nrow(df))
df <- df %>% select(ID, everything())
# finding which columns in df are categorical
cat_class <- as.data.frame(sapply(df, class))
cat_class
# exploring the unique data values for all categorical variables
cat_types <- c(3,4,5,6,7,8,9,10,11,16,22)
for(i in cat_types){
print(colnames(df[i]))
print(df %>% count(df[,i]))
}
df_stg <- df
df_stg[df_stg=="unknown"] <- NA
vis_miss(df_stg, warn_large_data = FALSE) + xlab("Data Columns")
# filtering out all NA values from dataset
df_stg <- df_stg %>% filter(!is.na(job) & !is.na(marital) & !is.na(education) & !is.na(default)
& !is.na(housing) & !is.na(loan) & !is.na(contact))
# validating reduction in row totals after filtering missing data
nrow(df)
nrow(df_stg)
# new df storing ID and all converted categorical variables into factor types
df_cat <- df_stg
for(i in cat_types){
df_cat[, i] <- factor(df_cat[,i])
x <- paste(colnames(df_cat[i]),"desc", sep="_")
colnames(df_cat)[i] <- paste(x)
}
# create new df containing ID and all converted categorical variables into numerical types
df_cat_num <- df_stg
for(i in cat_types){
df_cat_num[, i] <- as.numeric(factor(df_cat_num[,i]))
x <- paste(colnames(df_cat_num[i]),"ID", sep="_")
colnames(df_cat_num)[i] <- paste(x)
}
cat_columns <- c(1,3,4,5,6,7,8,9,10,11,16,22)
# 2 dfs, 1 storing all the IDs (df_cat_num_short), the other storying all the factored "descriptions" (df_cat_short)
df_cat_num_short <- data.frame(df_cat_num[, cat_columns])
df_cat_short <- data.frame(df_cat[, cat_columns])
# storying all numerical data into temp df excluding categorical types
df_num_only <- df[, -cat_types]
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
df_cat_merge <- df_cat_merge[c(1,2,13,3,14,4,15,5,16,6,17,7,18,8,19,9,20,10,21,11,22,12,23)]
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
df_num_only <- df[, -cat_types]
df_cat_merge <- merge(x=df_cat_num_short, y=df_cat_short, by="ID", all.x=TRUE)
str(df_cat_num_short)
library(tidyr)
library(dplyr)
library(ggplot2)
library(tidyverse)
setwd("C:/Users/David/Google Drive/Code & Programming/GitHub/MSDS_6372_Applied-Statistics_Project_2/UCI_Bank_Marketing_Classification_Model/Data Files/")
df <- read.csv("bank-additional-full.csv", header=TRUE, sep=";")
str(df)
df <- df %>% rename(clnt.subcr = `y`)
library(naniar)
# adding sequential ID column to all rows
df$ID <- seq.int(nrow(df))
df <- df %>% select(ID, everything())
# adding sequential ID column to all rows
df$ID <- seq.int(nrow(df))
df <- df %>% select(ID, everything())
